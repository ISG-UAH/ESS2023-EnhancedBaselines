{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usually not recommended, but they simply add unneeded verbosity to the output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as scp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.linear_model import *\n",
    "from sklearn.svm import *\n",
    "import sympy as sp\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offsets are expressed in days\n",
    "offset_1 =1\n",
    "offset_2 = 24\n",
    "model_type = 'elasticnet_AR_scaled'\n",
    "if offset_2:\n",
    "    model = f'{model_type}_{offset_1}d_{offset_2}d'\n",
    "else:\n",
    "    model = f'{model_type}_{offset_1}d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correspondences = {\n",
    "    'V': 'V_t',\n",
    "    'N': '\\\\rho_t',\n",
    "    'T': 'T_t',\n",
    "    'BT': 'B_T',\n",
    "    'BR': 'B_R',\n",
    "    'BN': 'B_N', \n",
    "}\n",
    "\n",
    "def parse_name(n):\n",
    "    new = n.split('-')\n",
    "    return new[0]+f'_t-{new[1]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabulate_series(df, freq='1h', inputs=1, offset=1, second_offset = None):\n",
    "    '''\n",
    "        Tabulate time series data contained in a matrix to generate vectors usable by\n",
    "        the algorithms included in this work.\n",
    "    '''\n",
    "    src = df.copy().shift(offset+inputs)\n",
    "    new_c = src.columns.values.copy()\n",
    "    for i in range(len(new_c)):\n",
    "        new_c[i] = new_c[i]+f'-{offset+inputs}'\n",
    "    src.columns = new_c\n",
    "    for i in tqdm(range(offset+inputs-1, offset-1, -1), desc='Data tabulation (1st pass)'):\n",
    "        new_df = df.shift(i, freq=freq)\n",
    "        new_c = new_df.columns.values.copy()\n",
    "        new_df.columns = new_c\n",
    "        for j in range(len(new_c)):\n",
    "            new_c[j] = new_c[j]+f'-{i}'\n",
    "        src = src.join(new_df)\n",
    "    if second_offset:\n",
    "        for i in tqdm(range(second_offset+inputs-1, second_offset-1, -1), desc='Data tabulation (2nd pass)'):\n",
    "            new_df = df.shift(i, freq=freq)\n",
    "            new_c = new_df.columns.values.copy()\n",
    "            new_df.columns = new_c\n",
    "            for j in range(len(new_c)):\n",
    "                new_c[j] = new_c[j]+f'-{i}'\n",
    "            src = src.join(new_df)\n",
    "    return src.join(df).dropna(how='any')\n",
    "\n",
    "# Support functions to enable generation of the correct J and K values.\n",
    "# Do not modify.\n",
    "def compute_num_inputs(days=1, freq='1d'):\n",
    "    if 'd' in freq:\n",
    "        divisor = float(freq.replace('d', ''))\n",
    "    if 'h' in freq:\n",
    "        divisor = float(freq.replace('h', ''))/24\n",
    "    if 'min' in freq:\n",
    "        divisor = float(freq.replace('min', ''))/(24*60)\n",
    "    if 's' in freq:\n",
    "        divisor = float(freq.replace('s', ''))/(24*60*60)\n",
    "    return round(days/divisor)\n",
    "\n",
    "def compute_window_size(freq='1d'):\n",
    "    if 'd' in freq:\n",
    "        return float(freq.replace('d', ''))*24\n",
    "    else:\n",
    "        return float(freq.replace('h', ''))\n",
    "\n",
    "# Main algorithm\n",
    "def explore_aggregation_space(aggregations, min_input_days=1, max_input_days=5, \n",
    "                              min_offset_days=1, max_offset_days=4, special_offset = None):\n",
    "    '''\n",
    "        Algorithmically generate as many configurations as needed for the selected model.\n",
    "        To change the model use, simply comment the one currently enabled an uncomment the\n",
    "        selected sklearn model.\n",
    "    '''\n",
    "    error_metrics = {}\n",
    "    expressions = {'l1_ratio': []}\n",
    "    expressions['latex'] = []\n",
    "    for agg in tqdm(aggregations, desc='Aggregation sizes explored'):\n",
    "        # prepare dictionary for error metric indexing using pandas later\n",
    "        error_metrics[f'{agg}_rmse'] = {}\n",
    "        error_metrics[f'{agg}_mae'] = {}\n",
    "        error_metrics[f'{agg}_corr'] = {}\n",
    "        error_metrics[f'{agg}_r2'] = {}\n",
    "        \n",
    "        # generate a copy of the original dataset exclusive to the loop iteration\n",
    "        surrogate_data = wind_data.copy()\n",
    "        surrogate_data_non_smoothed = surrogate_data.copy() # set aside an unsmoothed copy of data\n",
    "        # aggregate by time\n",
    "        surrogate_data = surrogate_data.rolling(round(compute_window_size(agg)), center=True).mean().dropna(how='any')\n",
    "        \n",
    "        # now we loop over the different configurations for the model\n",
    "        for inpt in tqdm(range(min_input_days, max_input_days+1), \n",
    "                           desc=f'Number of days in input for {agg} aggregation explored'):\n",
    "            input_size = compute_num_inputs(inpt, agg)\n",
    "            for offset in tqdm(range(min_offset_days, max_offset_days+1), desc='Offsets explored'):\n",
    "                offset_size = offset*24\n",
    "\n",
    "                # tabulate data for usage according to our configuration\n",
    "                if special_offset:\n",
    "                    tabulated = tabulate_series(surrogate_data, agg, input_size, offset_size, special_offset*24)\n",
    "                    tabulated_non_smoothed = tabulated#tabulate_series(surrogate_data_non_smoothed, agg, input_size, offset_size,\n",
    "#                                                             special_offset*24)\n",
    "                else:\n",
    "                    tabulated = tabulate_series(surrogate_data, agg, input_size, offset_size)\n",
    "                    tabulated_non_smoothed = tabulated#tabulate_series(surrogate_data_non_smoothed, agg, input_size, offset_size)\n",
    "                    \n",
    "                \n",
    "                # exclude specific columns from potential inputs\n",
    "                exclude = ['lower_loc', 'upper_loc', 'height', 'mag_skewness', 'mag_flux', 'BR', 'BT', 'BN', 'unipolarity',\n",
    "                          'N', 'T', 'log_{10}T']\n",
    "                inputs = tabulated.columns.values.tolist()\n",
    "                # very dirty exception handling, but we know there will only be\n",
    "                # exceptions here if the column doesn't exist, so we can just\n",
    "                # skip any that are like that and keep going until we're done with the\n",
    "                # entire list\n",
    "                for o in tqdm(targets, desc='Filtering targets out from input dataset'):\n",
    "                    try:\n",
    "                        inputs.remove(o)\n",
    "                    except:\n",
    "                        continue\n",
    "                for o in tqdm(exclude, desc='Removing banned entries from input dataset'):\n",
    "                    try:\n",
    "                        inputs.remove(o)\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # split tabulated data between inputs and targets\n",
    "                input_df = tabulated[inputs]\n",
    "                output_df = tabulated[targets]\n",
    "                \n",
    "                test_data_in = []\n",
    "                test_data_out = []\n",
    "                for row in tqdm(val_info.values, desc='Generating dataset split'):\n",
    "                    start = row[0]\n",
    "                    end = row[1]\n",
    "                    new_df_in = input_df[input_df.index >= start].copy()\n",
    "                    new_df_in = new_df_in[new_df_in.index <= end].copy()\n",
    "                    new_df_out = tabulated_non_smoothed[targets][tabulated_non_smoothed.index >= start].copy()\n",
    "                    new_df_out = new_df_out[new_df_out.index <= end].copy()\n",
    "                    test_data_in.append(new_df_in)\n",
    "                    test_data_out.append(new_df_out)\n",
    "                \n",
    "                inpt_scaler = MinMaxScaler()\n",
    "                outpt_scaler = MinMaxScaler()\n",
    "                    \n",
    "                X_test = pd.concat(test_data_in)\n",
    "                y_test = pd.concat(test_data_out)\n",
    "                X_train = input_df[~input_df.index.isin(X_test.index)].copy()\n",
    "                y_train = output_df[~output_df.index.isin(X_test.index)].copy()\n",
    "                \n",
    "                \n",
    "                inpt_scaler.fit(X_train)\n",
    "                outpt_scaler.fit(y_train)\n",
    "                \n",
    "                X_train = pd.DataFrame(inpt_scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "                y_train = pd.DataFrame(outpt_scaler.transform(y_train), index=y_train.index, columns=y_train.columns)\n",
    "                X_test = pd.DataFrame(inpt_scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n",
    "                \n",
    "                model = ElasticNetCV(l1_ratio=[i for i in np.arange(0.1, 1.0, .03)], \n",
    "                                     verbose=0, \n",
    "                                     n_jobs=2)\n",
    "#                 model = SVR(kernel='rbf')\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "#                 try:\n",
    "#                     print(f'Support vectors: {model.support_vectors_}')\n",
    "#                 except:\n",
    "#                     pass\n",
    "    \n",
    "                try:\n",
    "                    expressions['l1_ratio'].append(model.l1_ratio_)\n",
    "                except Exception as e:\n",
    "                    print(f'Exception when extracting alpha parameter for {inpt} inputs, {offset} outputs case:\\n\\t{e}')\n",
    "                    expressions['l1_ratio'].append(np.nan)\n",
    "                pred = outpt_scaler.inverse_transform(model.predict(X_test).reshape(-1, 1))\n",
    "                #pred = model.predict(X_test).reshape(-1, 1)\n",
    "                pred = 10**pred\n",
    "                y_test = 10**y_test.values[:, 0]\n",
    "                r2 = r2_score(y_test, pred)\n",
    "                rmse = mean_squared_error(y_test, pred, squared=False)\n",
    "                mae = mean_absolute_error(y_test, pred)\n",
    "                pearson, p = pearsonr(y_test, pred)\n",
    "                \n",
    "                del X_train\n",
    "                del y_train\n",
    "                del X_test\n",
    "                del y_test\n",
    "                del tabulated\n",
    "                if not tabulated_non_smoothed is None:\n",
    "                    del tabulated_non_smoothed\n",
    "                \n",
    "                error_metrics[f'{agg}_rmse'][f'{(inpt, offset)}'] = rmse\n",
    "                error_metrics[f'{agg}_mae'][f'{(inpt, offset)}'] = mae\n",
    "                error_metrics[f'{agg}_corr'][f'{(inpt, offset)}'] = pearson\n",
    "                error_metrics[f'{agg}_r2'][f'{(inpt, offset)}'] = r2\n",
    "               \n",
    "                try:\n",
    "                    exprs = {}\n",
    "                    for t in range(len(targets)):\n",
    "                        symb_expr = ''\n",
    "                        for s in model.feature_names_in_:\n",
    "                            try:\n",
    "                                symb_expr += correspondences[s]+' '\n",
    "                            except:\n",
    "                                symb_expr += parse_name(s)+' '\n",
    "                        symbols = sp.symbols(symb_expr)\n",
    "                        coefs = []\n",
    "                        mcoefs = model.coef_\n",
    "                        for i in range(len(mcoefs)):\n",
    "                            coefs.append(mcoefs[i]*symbols[i])\n",
    "                        expr = coefs[0]\n",
    "                        for c in coefs[1:]:\n",
    "                            expr += c\n",
    "                        expr += model.intercept_\n",
    "                        exprs[targets[t]] = expr\n",
    "                    for s, exp in exprs.items():\n",
    "                        expressions['latex'].append(sp.latex(sp.Eq(sp.symbols(correspondences[s]), exp.simplify())))\n",
    "                except Exception as e:\n",
    "                    print(f'Exception when extracting equation parameter for {inpt} inputs, {offset} outputs case:\\n\\t{e}')\n",
    "                    expressions['latex'].append('-')\n",
    "    \n",
    "    return error_metrics, expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV with validation info. Provided under the 'data' folder\n",
    "val_info = pd.read_csv('data/SW_comparison_validation_info.csv', sep=';')[['Start date', 'End date']]\n",
    "val_info = val_info[len(val_info)//2:]\n",
    "val_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with measurements, and ensure no element is out of range.\n",
    "# Data should be retrieved from OmniWeb and placed under the 'data' folder.\n",
    "# The dataset used is provided by default.\n",
    "wind_data = pd.read_csv('data/omni_1hr_clean.csv', index_col=0, parse_dates=[0])\n",
    "wind_data['V'] = wind_data['V'].apply(np.log10)\n",
    "wind_data['T'] = wind_data['T'].apply(np.log10)\n",
    "wind_data = wind_data[wind_data.index >= '2011-01-01']\n",
    "wind_data = wind_data[wind_data.index < '2018-01-01']\n",
    "display(wind_data)\n",
    "new_idx = pd.date_range(wind_data.index[0], wind_data.index[-1], freq='1h')\n",
    "wind_data = wind_data.reindex(new_idx, fill_value=np.nan)\n",
    "wind_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregations=['1h']\n",
    "targets = ['V']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if offset_2:\n",
    "    error_metrics, expressions = explore_aggregation_space(aggregations, min_offset_days=offset_1, \n",
    "                                                           max_offset_days=offset_1+2, special_offset=offset_2,\n",
    "                                                          max_input_days=5)\n",
    "else:    \n",
    "    error_metrics, expressions = explore_aggregation_space(aggregations, min_offset_days=offset_1, \n",
    "                                                           max_offset_days=offset_1+2,\n",
    "                                                          max_input_days=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_df = pd.DataFrame(error_metrics)\n",
    "em_df['1h_corr'] = em_df['1h_corr'].apply(lambda x: x[0])\n",
    "em_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['rmse', 'mae', 'corr', 'r2']\n",
    "metric_translations = {\n",
    "    'rmse': 'RMSE (km/s)',\n",
    "    'mae': 'MAE (km/s)',\n",
    "    'corr': 'Pearson\\'s correlation coefficient',\n",
    "    'r2': 'R$^{2}$'\n",
    "}\n",
    "\n",
    "aggregation_translation = {\n",
    "    '1d': '1 day',\n",
    "    '22h': '22 hours',\n",
    "    '20h': '20 hours',\n",
    "    '18h': '18 hours',\n",
    "    '16h': '16 hours',\n",
    "    '14h': '14 hours',\n",
    "    '12h': '12 hours',\n",
    "    '10h': '10 hours',\n",
    "    '8h': '8 hours',\n",
    "    '6h': '6 hours',\n",
    "    '4h': '4 hours',\n",
    "    '3h': '3 hours',\n",
    "    '2h': '2 hours',\n",
    "    '1h': '1 hour'\n",
    "}\n",
    "\n",
    "for m in metrics:\n",
    "    try:\n",
    "        select = []\n",
    "        for a in aggregations:\n",
    "            select.append(f'{a}_{m}')\n",
    "        plt.figure(figsize=(15, 5.5))\n",
    "        xlabels = []\n",
    "        for c in em_df[select].columns:\n",
    "            xlabels.append(aggregation_translation[c.split('_')[0]])\n",
    "        sns.heatmap(em_df[select], annot=True, xticklabels=xlabels)\n",
    "        plt.title(f'Comparison of {metric_translations[m]} per configuration')\n",
    "        plt.xlabel('Aggregation size')\n",
    "        plt.ylabel('Input and offset length (days)')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = []\n",
    "for agg in aggregations:\n",
    "    for v in em_df.index.values:\n",
    "        idx.append(f'{agg}_{v}')\n",
    "expr_df = pd.DataFrame(expressions, index=idx)\n",
    "expr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_df.groupby('l1_ratio').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(expr_df['l1_ratio'], kde=True)\n",
    "plt.suptitle('Number of expressions per L1 ratio')\n",
    "plt.title(f'(N = {len(expr_df)})')\n",
    "plt.xlabel('L1 ratio')\n",
    "plt.show()\n",
    "plt.close()\n",
    "sns.histplot(expr_df['l1_ratio'], kde=True, stat='probability')\n",
    "plt.suptitle('Probability of expressions per L1 ratio')\n",
    "plt.title(f'(N = {len(expr_df)})')\n",
    "plt.xlabel('L1 ratio')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_df.to_csv(f'results/performance_metrics_{model}.csv')\n",
    "expr_df.to_csv(f'results/expressions_{model}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
